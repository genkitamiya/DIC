{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 深層学習スクラッチ リカレントニューラルネットワーク\n",
    "リカレントニューラルネットワーク（RNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。<br>\n",
    "<br>\n",
    "フォワードプロパゲーションの実装を必須課題とし、バックプロパゲーションの実装はアドバンス課題とします。<br>\n",
    "<br>\n",
    "クラスの名前はScratchSimpleRNNClassifierとしてください。クラスの構造などは以前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】SimpleRNNのフォワードプロパゲーション実装\n",
    "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。<br>\n",
    "<br>\n",
    "フォワードプロパゲーションの数式は以下のようになります。ndarrayのshapeがどうなるかを併記しています。<br>\n",
    "<br>\n",
    "バッチサイズをbatch_size、入力の特徴量数をn_features、RNNのノード数をn_nodesとして表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。\n",
    "\n",
    "$$\n",
    "a_t = x_{t}\\cdot W_{x} + h_{t-1}\\cdot W_{h} + B\\\\\n",
    "h_t = tanh(a_t)\n",
    "$$\n",
    "\n",
    "$a_t$ : 時刻tの活性化関数を通す前の状態 (batch_size, n_nodes)\n",
    "\n",
    "$h_t$ : 時刻tの状態・出力 (batch_size, n_nodes)\n",
    "\n",
    "$x_{t}$ : 時刻tの入力 (batch_size, n_features)\n",
    "\n",
    "$W_{x}$ : 入力に対する重み (n_features, n_nodes)\n",
    "\n",
    "$h_{t-1}$ : 時刻t-1の状態（前の時刻から伝わる順伝播） (batch_size, n_nodes)\n",
    "\n",
    "$W_{h}$ : 状態に対する重み。 (n_nodes, n_nodes)\n",
    "\n",
    "$B$ : バイアス項 (n_nodes,)\n",
    "\n",
    "初期状態$h_{0}$は全て0とすることが多いですが、任意の値を与えることも可能です。<br>\n",
    "<br>\n",
    "上記の処理を系列数n_sequences回繰り返すことになります。RNN全体への入力$x$は(batch_size, n_sequences, n_features)のような配列で渡されることになり、そこから各時刻の配列を取り出していきます。<br>\n",
    "<br>\n",
    "分類問題であれば、それぞれの時刻のhに対して全結合層とソフトマックス関数（またはシグモイド関数）を使用します。タスクによっては最後の時刻のhだけを使用することもあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    層の生成\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    activation : object instance\n",
    "      活性化関数インスタンス\n",
    "    optimizer : object instance\n",
    "      最適化手法のインスタンス\n",
    "      \n",
    "    Attributes\n",
    "    ----------\n",
    "    w : 次の形のtuple, shape (Wx, Wh)\n",
    "      重みパラメータ\n",
    "    b : 次の形のndarray, shape (n_nodes, )\n",
    "      バイアスパラメータ\n",
    "    \"\"\"\n",
    "    def __init__(self, activation, optimizer=None, weights=None, intercept=None):\n",
    "        self.activ = activation\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.w = weights\n",
    "        self.b = intercept\n",
    "\n",
    "    \n",
    "    def forward(self, X, h):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_features)\n",
    "            入力\n",
    "        h : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            前の層の入力（t-1の状態）\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        H : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力（tの状態）\n",
    "        \"\"\"      \n",
    "        W_x = self.w[0]\n",
    "        W_h = self.w[1]\n",
    "        \n",
    "        A = X@W_x + h@W_h + self.b\n",
    "        H = self.activ.forward(A)\n",
    "\n",
    "        self.X = X[None,:]\n",
    "        self.h = h\n",
    "        self.output = H\n",
    "        self.A_ = A\n",
    "\n",
    "        return H\n",
    "    \n",
    "\n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    ハイパーボリックタンジェント関数\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Z_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
    "      順伝播の出力\n",
    "    dA_ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
    "      逆伝播入力に対するdA勾配\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.Z_ = None\n",
    "        self.dA_ = None\n",
    "        \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
    "          ある層に順伝播された特徴量データ\n",
    "        \"\"\"\n",
    "        self.Z_ = np.tanh(A)\n",
    "        \n",
    "        return self.Z_\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
    "          ある層に逆伝播されたZに関するLoss勾配\n",
    "        \"\"\"\n",
    "        self.dA_ = dZ * (1 - self.Z_**2)\n",
    "        \n",
    "        return self.dA_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】小さな配列でのフォワードプロパゲーションの実験\n",
    "小さな配列でフォワードプロパゲーションを考えてみます。\n",
    "\n",
    "入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n",
    "\n",
    "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。\n",
    "\n",
    "```\n",
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
    "```\n",
    "\n",
    "フォワードプロパゲーションの出力が次のようになることを作成したコードで確認してください。\n",
    "\n",
    "```\n",
    "h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]]) # (batch_size, n_nodes)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手計算\n",
    "H1 = np.tanh(x[0,0]@w_x + h@w_h + b)\n",
    "H2 = np.tanh(x[0,1]@w_x + H1@w_h + b)\n",
    "H3 = np.tanh(x[0,2]@w_x + H2@w_h + b)\n",
    "H3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
     ]
    }
   ],
   "source": [
    "W = (w_x, w_h)\n",
    "B = b\n",
    "rnn = SimpleRNN(Tanh(), weights=W, intercept=B)\n",
    "\n",
    "for s in range(n_sequences):\n",
    "    # 順伝播\n",
    "    H = rnn.forward(x[0,s], h)\n",
    "    h = H\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.57790736e-10,  3.93828459e-09, -1.37243750e-09,\n",
       "        -1.88850646e-09]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_true = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]]) # (batch_size, n_nodes)\n",
    "H - h_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】（アドバンス課題）バックプロパゲーションの実装\n",
    "バックプロパゲーションを実装してください。\n",
    "\n",
    "RNNの内部は全結合層を組み合わせた形になっているので、更新式は全結合層などと同様です。\n",
    "\n",
    "$$\n",
    "W_x^{\\prime} = W_x - \\alpha \\frac{\\partial L}{\\partial W_x} \\\\\n",
    "W_h^{\\prime} = W_h - \\alpha \\frac{\\partial L}{\\partial W_h} \\\\\n",
    "B^{\\prime} = B - \\alpha \\frac{\\partial L}{\\partial B}\n",
    "$$\n",
    "\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_x}$ : $W_x$に関する損失$L$の勾配\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_h}$ : $W_h$に関する損失$L$の勾配\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B}$ : $B$に関する損失$L$の勾配<br>\n",
    "<br>\n",
    "勾配を求めるためのバックプロパゲーションの数式が以下です。<br>\n",
    "\n",
    "$\\frac{\\partial h_t}{\\partial a_t}$ = $\\frac{\\partial L}{\\partial h_t} × (1-tanh^2(a_t))$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B}$ = $\\frac{\\partial h_t}{\\partial a_t}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_x}$ = $x_{t}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_h}$ = $h_{t-1}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}$<br>\n",
    "<br>\n",
    "＊$\\frac{\\partial L}{\\partial h_t}$は前の時刻からの状態の誤差と出力の誤差の合計です。hは順伝播時に出力と次の層に伝わる状態双方に使われているからです。\n",
    "\n",
    "前の時刻や層に流す誤差の数式は以下です。\n",
    "\n",
    "$\\frac{\\partial L}{\\partial h_{t-1}}$ = $\\frac{\\partial h_t}{\\partial a_t}\\cdot W_{h}^{T}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_{t}}$ = $\\frac{\\partial h_t}{\\partial a_t}\\cdot W_{x}^{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    層の生成\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    activation : object instance\n",
    "      活性化関数インスタンス\n",
    "    optimizer : object instance\n",
    "      最適化手法のインスタンス\n",
    "      \n",
    "    Attributes\n",
    "    ----------\n",
    "    w : 次の形のtuple, shape (Wx, Wh)\n",
    "      重みパラメータ\n",
    "    b : 次の形のndarray, shape (n_nodes, )\n",
    "      バイアスパラメータ\n",
    "    \"\"\"\n",
    "    def __init__(self, activation, optimizer=None, weights=None, intercept=None):\n",
    "        self.activ = activation\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.w = weights\n",
    "        self.b = intercept\n",
    "        \n",
    "        \n",
    "    def forward(self, X, h):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_features)\n",
    "            入力\n",
    "        h : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            前の層の入力（t-1の状態）\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        H : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力（tの状態）\n",
    "        \"\"\"      \n",
    "        W_x = self.w[0]\n",
    "        W_h = self.w[1]\n",
    "        \n",
    "        A = X@W_x + h@W_h + self.b\n",
    "        H = self.activ.forward(A)\n",
    "\n",
    "        self.X = X[None,:]\n",
    "        self.h = h\n",
    "        self.output = H\n",
    "        self.A_ = A\n",
    "\n",
    "        return H\n",
    "    \n",
    " \n",
    "    def backward(self, dH, dW, dB):\n",
    "        \"\"\"\n",
    "        逆伝播（Back Propagation Through Time）\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dH : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
    "          後ろから流れてきた勾配\n",
    "        dW : 次の形のtuple, shape (dWx, dWh)\n",
    "          一つ後ろの状態の重み勾配\n",
    "        dB : 次の形のndarray, shape (n_nodes, )\n",
    "          一つ後ろの状態の重み勾配\n",
    "        lr : float\n",
    "          学習率\n",
    "        \"\"\"\n",
    "        w_x = self.w[0]\n",
    "        w_h = self.w[1]\n",
    "        \n",
    "        dA = self.activ.backward(dH)\n",
    "        dh = dA@w_h.T\n",
    "        dWx = self.X.T @ dA\n",
    "        dWh = self.h.T @ dA\n",
    "    \n",
    "        dW[0] += dWx\n",
    "        dW[1] += dWh\n",
    "        dB += dA\n",
    "        \n",
    "        self.dH = dH\n",
    "        self.dW = dW\n",
    "        self.dB = dB\n",
    "        \n",
    "        return dh, dW, dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
     ]
    }
   ],
   "source": [
    "W = (w_x, w_h)\n",
    "B = b\n",
    "\n",
    "network = []\n",
    "output = []\n",
    "for s in range(n_sequences):\n",
    "    network.append(SimpleRNN(Tanh(), weights=W, intercept=B))\n",
    "    # 順伝播\n",
    "    h = network[s].forward(x[0,s], h)\n",
    "    output.append(h)\n",
    "    \n",
    "print(output[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "(2, 4)\n",
      "(4, 4)\n",
      "(1, 4)\n"
     ]
    }
   ],
   "source": [
    "dW = [np.zeros(w_x.shape), np.zeros(w_h.shape)]\n",
    "dB = np.zeros(b.shape)[None,:]\n",
    "lr = 0\n",
    "for s in range(n_sequences):\n",
    "    # 逆伝播\n",
    "    dH, dW, dB = network[n_sequences-s-1].backward(output[-1], dW, dB)\n",
    "print(dH.shape)\n",
    "print(dW[0].shape)\n",
    "print(dW[1].shape)\n",
    "print(dB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleRNNClassifier\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "\n",
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    層の生成\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    activation : object instance\n",
    "      活性化関数インスタンス\n",
    "    optimizer : object instance\n",
    "      最適化手法のインスタンス\n",
    "      \n",
    "    Attributes\n",
    "    ----------\n",
    "    w : 次の形のtuple, shape (Wx, Wh)\n",
    "      重みパラメータ\n",
    "    b : 次の形のndarray, shape (n_nodes, )\n",
    "      バイアスパラメータ\n",
    "    \"\"\"\n",
    "    def __init__(self, activation, optimizer=None, weights=None, intercept=None):\n",
    "        self.activ = activation\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.w = weights\n",
    "        self.b = intercept\n",
    "        \n",
    "        \n",
    "    def forward(self, X, h):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_features)\n",
    "            入力\n",
    "        h : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            前の層の入力（t-1の状態）\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        H : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力（tの状態）\n",
    "        \"\"\"      \n",
    "        W_x = self.w[0]\n",
    "        W_h = self.w[1]\n",
    "        \n",
    "        A = X@W_x + h@W_h + self.b\n",
    "        H = self.activ.forward(A)\n",
    "\n",
    "        self.X = X[None,:]\n",
    "        self.h = h\n",
    "        self.output = H\n",
    "        self.A_ = A\n",
    "\n",
    "        return H\n",
    "    \n",
    " \n",
    "    def backward(self, dH, dW, dB):\n",
    "        \"\"\"\n",
    "        逆伝播（Back Propagation Through Time）\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dH : 次の形のndarray, shape (batch_size, n_nodes_self)\n",
    "          後ろから流れてきた勾配\n",
    "        dW : 次の形のtuple, shape (dWx, dWh)\n",
    "          一つ後ろの状態の重み勾配\n",
    "        dB : 次の形のndarray, shape (n_nodes, )\n",
    "          一つ後ろの状態の重み勾配\n",
    "        lr : float\n",
    "          学習率\n",
    "        \"\"\"\n",
    "        w_x = self.w[0]\n",
    "        w_h = self.w[1]\n",
    "        \n",
    "        dA = self.activ.backward(dH)\n",
    "        dh = dA@w_h.T\n",
    "        dWx = self.X.T @ dA\n",
    "        dWh = self.h.T @ dA\n",
    "    \n",
    "        dW[0] += dWx\n",
    "        dW[1] += dWh\n",
    "        dB += dA\n",
    "        \n",
    "        self.dH = dH\n",
    "        self.dW = dW\n",
    "        self.dB = dB\n",
    "        \n",
    "        return dh, dW, dB\n",
    "\n",
    "class ScratchSimpleRNNClassifier:\n",
    "    \"\"\"\n",
    "    可変層畳み込みニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    layers : list\n",
    "      ネットワークに組み込まれる層のリスト\n",
    "    epoch : int\n",
    "      エポック数\n",
    "    sigma : float\n",
    "      初期パラメータ用（SimpleInitializerのみ適用）\n",
    "    batch_size : int\n",
    "      ミニバッチのサンプル数\n",
    "    verbose : bool\n",
    "      学習経過の出力\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    loss_train : list\n",
    "      訓練データに対するLoss\n",
    "    loss_val : list\n",
    "      検証データに対するLoss\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, epoch=100, h_init=0, lr=0.01, batch_size=100, verbose=False, **kwargs):\n",
    "        self.layers = layers\n",
    "        self.epoch = epoch\n",
    "        self.lr = lr\n",
    "        self.h_init = h_init\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "        self.wx = None\n",
    "        self.wh = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_sequences, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, n_classes)\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_sequences, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, n_classes)\n",
    "            検証データの正解値\n",
    "        \"\"\"  \n",
    "        # 回帰結合ネットワーク生成\n",
    "        n_sequences = len(X[1])\n",
    "        self.network = []\n",
    "        for i in range(n_sequences):\n",
    "            self.network.append(self.layers.deepcopy())\n",
    "        \n",
    "        # 勾配の初期化\n",
    "        dW = [np.zeros(self.layers.w[0].shape), np.zeros(self.layers.w[1].shape)]\n",
    "        dB = np.zeros(self.layers.b.shape)[None,:]\n",
    "        \n",
    "        for i in range(self.epoch):\n",
    "            \n",
    "            get_mini_batch_t = GetMiniBatch(X, y, batch_size=self.batch_size, seed=i)\n",
    "            \n",
    "            times = []\n",
    "            start = time.time()\n",
    "            \n",
    "            # 各mini batchの損失をリスト化\n",
    "            loss_batch_t = []\n",
    "            \n",
    "            for X_mini, y_mini in get_mini_batch_t:\n",
    "\n",
    "                # 初期Hを設定\n",
    "                h = self.h_init\n",
    "                for s in range(n_sequences):\n",
    "                    # 順伝播\n",
    "                    H = self.network[s].forward(x_mini[0,s], h)\n",
    "                    h = H\n",
    "                    # 逆伝播\n",
    "                    dH, dW, dB = network[n_sequences-s-1].backward(y_mini, dW, dB)\n",
    "\n",
    "                loss_batch_t.append(self.cross_entropy(output, y_mini))\n",
    "            \n",
    "                # パラメータ更新\n",
    "                self.wx -= lr * self.optimizer.update_dw(self, dW[0])\n",
    "                self.wh -= lr * self.optimizer.update_dw(self, dW[1])\n",
    "                self.b -= lr * self.optimizer.update_db(self, dB)\n",
    "            \n",
    "            \n",
    "            # 各epochの平均損失をselfに格納\n",
    "            loss_train = np.mean(loss_batch_t)\n",
    "            self.loss_train.append(loss_train)\n",
    "            \n",
    "            \n",
    "            # 検証データの推定\n",
    "            if hasattr(X_val, '__array__') and hasattr(y_val, '__array__'):\n",
    "                \n",
    "                batch_size_v = int(self.batch_size * len(X_val)/len(X))\n",
    "                get_mini_batch_v = GetMiniBatch(X_val, y_val, batch_size=batch_size_v)\n",
    "                loss_batch_v = []\n",
    "\n",
    "                for X_mini, y_mini in get_mini_batch_v:\n",
    "                    # 初期Hを設定\n",
    "                    h = self.h_init\n",
    "                    for s in range(n_sequences):\n",
    "                        # 順伝播\n",
    "                        H = self.network[s].forward(x_mini[0,s], h)\n",
    "                        h = H\n",
    "                        \n",
    "                    loss_batch_v.append(self.cross_entropy(H, y_mini))\n",
    "            \n",
    "                # 各epochの平均損失をselfに格納\n",
    "                loss_val = np.mean(loss_batch_v)\n",
    "                self.loss_val.append(loss_val)\n",
    "\n",
    "            end = time.time()\n",
    "            times.append(end-start)\n",
    "\n",
    "            # 学習経過の出力\n",
    "            if self.verbose and (i+1) % 10 == 0:\n",
    "                print(\"Epoch {}; Loss {:.4f}\".format(i+1, loss_train),\n",
    "                      \"  --Avg Epoch Time {:.4f}sec\".format(np.mean(times)))            \n",
    "                   \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          検証用データ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          次の形のndarray, shape (n_samples, )\n",
    "          推定結果\n",
    "        \"\"\"\n",
    "        n_sequences = len(X[1])\n",
    "        # 初期Hを設定\n",
    "        h = self.h_init\n",
    "        output = []\n",
    "        for s in range(n_sequences):\n",
    "            H = self.network[s].forward(x[0,s], h)\n",
    "            h = H        \n",
    "            output.append(H)\n",
    "                \n",
    "        return H\n",
    "        \n",
    "    def cross_entropy(self, X, y):\n",
    "        \"\"\"\n",
    "        クロスエントロピー誤差を計算\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_features)\n",
    "          入力データ\n",
    "        y : 次の形のndarray, shape (batch_size, n_classes)\n",
    "          入力データの正解ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          float\n",
    "          クロスエントロピー誤差\n",
    "        \"\"\"\n",
    "        return (-1/len(X)) * np.sum((y*np.log(X)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
