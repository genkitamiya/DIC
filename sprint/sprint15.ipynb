{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 論文読解入門\n",
    "\n",
    "以下の論文を読み問題に答えてください。CNNを使った物体検出（Object Detection）の代表的な研究です。<br>\n",
    "\n",
    "[8]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. (2015) 91–99<br>\n",
    "<br>\n",
    "[https://arxiv.org/pdf/1506.01497.pdf](https://arxiv.org/pdf/1506.01497.pdf)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題\n",
    "それぞれについてJupyter Notebookにマークダウン形式で記述してください。\n",
    "\n",
    "1. 物体検出の分野にはどういった手法が存在したか。\n",
    "2. Fasterとあるが、どういった仕組みで高速化したのか。\n",
    "3. One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    "4. RPNとは何か。\n",
    "5. RoIプーリングとは何か。\n",
    "6. Anchorのサイズはどうするのが適切か。\n",
    "7. 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    "8. （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。\n",
    "\n",
    "<br>\n",
    "**条件**<br>\n",
    "答える際は論文のどの部分からそれが分かるかを書く。<br>\n",
    "必要に応じて先行研究（引用されている論文）も探しにいく。最低2つは他の論文を利用して回答すること。<br>\n",
    "論文の紹介記事を見ても良い。ただし、答えは論文内に根拠を探すこと。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 物体検出の分野にはどういった手法が存在したか。\n",
    "> 物体検出を行うモデルにはR-CNN(Regional)やSPPnet(Spatial Pyramid Pooling Networks)など、領域候補（Region Proposal）の抽出する手法が採用されている。<br>\n",
    "> (Recent advances in object detection are driven by the success of region proposal methods (e.g., [4]) and region-based convolutional neural networks (R- CNNs) [5]) 引用元 --R. Girshick, “Fast R-CNN,” in IEEE International Conference on Computer Vision (ICCV), 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fasterとあるが、どういった仕組みで高速化したのか。\n",
    "> 領域検出を行うFast-CNNなどのDeep-CNNから計算されるconvolutionsを物体検出に使用されるモデルと共有することによって、計算時間を短縮することに成功した。<br>\n",
    "> \\*引用元：p1 (...we introduce novel Region Proposal Networks (RPNs) that share convolutional layers with state-of-the-art object detection networks [1], [2]. By sharing convolutions at test-time, the marginal cost for computing proposals is small (e.g., 10ms per image).)<br>\n",
    ">\n",
    "> また、検出の際には、既存のマルチスケーリングや多数のフィルタサイズを使用するのではなく、anchor boxesを採用することによって、多くのサンプルやフィルタをiterateすることを回避でき、高速化につながった。<br>\n",
    "> \\*引用元：p1-p2 (In contrast to prevalent methods [8], [9], [1], [2] that use pyramids of images (Figure 1, a) or pyramids of filters (Figure 1, b), we introduce novel “anchor” boxes that serve as references at multiple scales and aspect ratios.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    "> One-stage Detection手法とは、feature map上をフィルタで探索し、領域候補の抽出及び検出物体の分類を同時に行う手法である。他方、Two-stage Proposalとは領域候補と分類を直列に連結された個別のモデルが行う手法である。両手法共スライド式のフィルタを用いているが、one-stageでは前述の通りフィルタの出力featuresで物体の位置及び分類を行う。Two-stageでのフィルタの役割はあくまで領域候補の抽出のみで、上位のモデル（Fast RNN）が出力された領域候補の精査・分類を行う。両手法の性能を比較したところ、two-stageはone-stageより約4.8%の精度向上を示し、計算時間もより早い。<br>\n",
    "> \\*引用元：p10 (The OverFeat paper [9] proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps. OverFeat is a one-stage, class-specific detection pipeline, and ours is a two-stage cascade consisting of class-agnostic pro- posals and class-specific detections.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. RPNとは何か。\n",
    "> Region Proposal Networksの略。画像にwindow/filterをスライドさせ、物体が含まれそうな領域を検出するモデル。画像またはフィルタのサイズを変更することによって、様々なサイズの領域抽出に対応する。推定された各領域にはobjectness score（物体が含まれる確率）によって評価される。<br>\n",
    "> \\*引用元：p3 (A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score.)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. RoIプーリングとは何か。\n",
    "> RoI Poolingとは、Fast R-CNNで実装された機構であり、抽出されたn個のRoI (Region of Interest)を固定されたサイズHxWのfeature mapにMaxPoolingされる。<br>\n",
    "> \\*引用元：R. Girshick, Fast R-CNN p1より(The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of H × W (e.g., 7 × 7), where H and W are layer hyper-parameters that are inde- pendent of any particular RoI)<br>\n",
    ">\n",
    "> 一方Faster R-CNNでは、Fast版と違ってRoIが固定の大きさなため、プーリング後のfeature map上でも全RoIが同サイズである。異なるRoIサイズに対応するため、RoIサイズひとつ毎にfeature mapを生成し、ひとつのfeature mapにつき回帰モジュール (regressors)を別に用意する。この手法によって、異なるRoIサイズ間にて重みが共有されることを回避できる。<br>\n",
    ">  \\*引用元：p5 (In our formula- tion, the features used for regression are of the same spatial size (3 × 3) on the feature maps. To account for varying sizes, a set of k bounding-box regressors are learned. Each regressor is responsible for one scale and one aspect ratio, and the k regressors do not share weights.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Anchorのサイズはどうするのが適切か。\n",
    "> 3つのanchor size（$128^2$, $256^2$, $512^2$）と１つまたは３つのアスペクト比（2:1, 1:1, 1:2）が採用データセット（PASCAL VOC 2007）に対して、最も高いmAP値を示した。アスペクト比の使用数はmAPに影響がなく、anchor sizeとアスペクト比に相関があると考察されている。ただ、モデルの汎化性を保つため、両パラメータを採用されている。<br>\n",
    "> \\*引用元：p6 (For anchors, we use 3 scales with box areas of 1282, 2562, and 5122 pixels, and 3 aspect ratios of 1:1, 1:2, and 2:1. These hyper-parameters are not carefully cho- sen for a particular dataset, and we provide ablation experiments on their effects in the next section.); p9 (The mAP is higher if using 3 scales (with 1 aspect ratio) or 3 aspect ratios (with 1 scale), demonstrating that using anchors of multiple sizes as the regression references is an effective solution. Using just 3 scales with 1 aspect ratio (69.8%) is as good as using 3 scales with 3 aspect ratios on this dataset, suggesting that scales and aspect ratios are not disentangled dimensions for the detection accuracy. But we still adopt these two dimensions in our designs to keep our system flexible.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    "> 本著ではPASCAL VOC 2007＋2012、及びMicrosoft COCOのデータセットが使用されている。両データセットとも過去にコンペで提供されたデータセットであり、PASCAL VOCは20種のカテゴリーを含むtrainvalデータが5000点、testデータが5000点あり、Microsoft COCOは80種のカテゴリーを含むtrain80,000点、validation40,000点、test20,000点が使用されている。先行研究との比較にはCOCOデータセットにて検証しており、testデータに対してFast R-CNNの評価がmAP@0.5: 39.3とmAP@[.5, .95]: 19.3だったところ、Faster R-CNNだとmAP@0.5: 42.1とmAP@[.5, .95]: 21.5であった。<br>\n",
    "> \\*引用元：p2 (We comprehensively evaluate our method on the PASCAL VOC 2007 detection benchmark [11]. This dataset consists of about 5k trainval images and 5k test images over 20 object categories.); p10 (This dataset involves 80 object categories. We experiment with the 80k images on the training set, 40k images on the validation set, and 20k images on the test-dev set. We evaluate the mAP averaged for IoU ∈ [0.5 : 0.05 : 0.95] (COCO’s standard metric, simply denoted as mAP@[.5, .95]) and mAP@0.5 (PASCAL VOC’s metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。\n",
    ">- K. He, G. Gkioxari, P. Dollár, and R. Girshick. Mask R-CNN. *arXiv:1703.06870*. 2018.\n",
    ">    - Faster R-CNNをInstance Segmentationに特化したモデルとして、Mask R-CNNが本著で提唱される。Faster R-CNNの構造を基本とし、各RoIに対する分類にインスタンスマスクを推定する機構が追加された。<br>\n",
    ">- S. Chou, C. Sun, W. Chang, W. Hsu, M. Sun, and J. Fu. 360-Indoor: Towards Learning Real-World Objects in 360◦ Indoor Equirectangular Images. *arXiv:1910.01712v1*. 2019\n",
    ">    - 360度画像データセットの物体検出を検証するモデルのひとつとして、Faster R-CNNが採用された。他のモデル同様、360度画像で学習したモデルの方が平面画像で学習したモデルより検出精度が高い結果となった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
