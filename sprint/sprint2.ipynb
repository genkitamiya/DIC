{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 機械学習スクラッチ入門\n",
    "このSprintでは機械学習手法のスクラッチ課題に取り組む準備を行います。scikit-learnを用いて分類・回帰問題を解くコードを書いておき、今後のSprintではそれと同じ動作をするクラスをスクラッチで作成していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### スクラッチの意義\n",
    "ここでのスクラッチとは、NumPyなどの基本的なライブラリを組み合わせることで、scikit-learnのような応用的なライブラリと同じ機能のクラス・関数を自作することを指します。<br>\n",
    "<br>\n",
    "スクラッチをすることでscikit-learnなどのライブラリを動かすだけでは掴みづらい、アルゴリズムの深い理解を目指します。コーディングのスキル向上も兼ねますが、それは主な目的ではありません。<br>\n",
    "<br>\n",
    "以下のような効果を狙っています。<br>\n",
    "\n",
    "- 新たな手法に出会った時に理論・数式を理解しやすくする\n",
    "- ライブラリを使う上での曖昧さを減らす\n",
    "- 既存の実装を読みやすくする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】train_test_splitのスクラッチ\n",
    "スクラッチの練習として、scikit-learnのtrain_test_splitを自作してみます。以下の雛形をベースとして関数を完成させてください。<br>\n",
    "<br>\n",
    "[sklearn.model_selection.train_test_split — scikit-learn 0.22.2 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)<br>\n",
    "<br>\n",
    "なお、作成した関数がscikit-learnのtrain_test_splitと同じ動作をしているか必ず確認をするようにしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scratch_train_test_split(X, y, train_size=0.8, stratify=True):\n",
    "    \"\"\"\n",
    "    検証データを分割する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, )\n",
    "      正解値\n",
    "    train_size : float (0<train_size<1)\n",
    "      何割をtrainとするか指定\n",
    "    stratify : bool\n",
    "      層化の有無を指定\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    X_train : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    X_test : 次の形のndarray, shape (n_samples, n_features)\n",
    "      検証データ\n",
    "    y_train : 次の形のndarray, shape (n_samples, )\n",
    "      訓練データの正解値\n",
    "    y_test : 次の形のndarray, shape (n_samples, )\n",
    "      検証データの正解値\n",
    "    \"\"\"\n",
    "    #ここにコードを書く\n",
    "    \n",
    "    #stratify機能を追加\n",
    "    #StratifiedShuffleSplitのソースコードを参考にした\n",
    "    if stratify:\n",
    "        labels, y_indices = np.unique(y, return_inverse=True)  #ラベルの種類と値をリストで取得\n",
    "        label_counts = np.bincount(y_indices)  #各種ラベルのサンプル数を取得\n",
    "        n_labels = len(labels)  #ラベルの種類数を取得\n",
    "        \n",
    "        #全てのサンプルにインデックスを与え、ラベル毎にリスト化\n",
    "        label_indices = np.split(np.argsort(y_indices, kind='mergesort'),\n",
    "                                 np.cumsum(label_counts)[:-1])\n",
    "        \n",
    "        train = []\n",
    "        test = []\n",
    "\n",
    "        #ラベル毎に下記を繰り返す\n",
    "        for i in range(n_labels):\n",
    "            #同一ラベル内で取得するサンプルをランダム化\n",
    "            permutation = np.random.permutation(label_counts[i])  \n",
    "            perm_indices_label_i = label_indices[i].take(permutation, mode='clip')\n",
    "\n",
    "            #分割率に合わせて、ラベルのインデックスをtrainとtestに振り分ける\n",
    "            split_length = round(label_counts[i]*train_size).astype(int)\n",
    "            train.extend(perm_indices_label_i[:split_length])\n",
    "            test.extend(perm_indices_label_i[split_length:])\n",
    "        \n",
    "        #インデックスに応じたサンプルを格納\n",
    "        X_train = X[train]\n",
    "        X_test = X[test]\n",
    "\n",
    "        y_train = y[train]\n",
    "        y_test = y[test]\n",
    "\n",
    "    else:\n",
    "        split_length = round(len(X)*train_size)\n",
    "    \n",
    "        X_train = X[:split_length]\n",
    "        X_test = X[split_length:]\n",
    "\n",
    "        y_train = y[:split_length]\n",
    "        y_test = y[split_length:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "スクラッチ版のX_trainデータ数：1027\n",
      "スクラッチ版のy_trainデータ数：1027\n",
      "\n",
      "\n",
      "sklearn版のX_trainデータ数：1027\n",
      "sklearn版のy_trainデータ数：1027\n"
     ]
    }
   ],
   "source": [
    "# スクラッチの動作確認\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ダミーデータ作成\n",
    "X = np.zeros((1284, 3))\n",
    "y = np.zeros((1284, 1))\n",
    "\n",
    "# スクラッチ関数によるtrainデータ数を確認\n",
    "X_train, X_test, y_train, y_test = scratch_train_test_split(X, y)\n",
    "\n",
    "print(\"スクラッチ版のX_trainデータ数：{}\".format(len(X_train)))\n",
    "print(\"スクラッチ版のy_trainデータ数：{}\".format(len(y_train)))\n",
    "print(\"\\n\")\n",
    "\n",
    "# sklearn関数によるtrainデータ数を確認\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
    "print(\"sklearn版のX_trainデータ数：{}\".format(len(X_train)))\n",
    "print(\"sklearn版のy_trainデータ数：{}\".format(len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learnを使ったコードを作成していきます。<br>\n",
    "<br>\n",
    "検証データの分割には問題1で作成した自作の関数を用いてください。クロスバリデーションではなくホールドアウト法で構いません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分類問題\n",
    "\n",
    "分類は3種類の手法をスクラッチします。<br>\n",
    "\n",
    "- ロジスティック回帰\n",
    "- SVM\n",
    "- 決定木<br>\n",
    "\n",
    "<br>\n",
    "ロジスティック回帰はscikit-learnにおいてLogisticRegressionクラスとSGDClassifierクラスの2種類から使用できます。ここでは勾配降下法を用いて計算するSGDClassifierクラスを利用してください。引数でloss=\"log\"とすることでロジスティック回帰の計算になります。<br>\n",
    "<br>\n",
    "データセットは3種類用意します。<br>\n",
    "<br>\n",
    "1つ目は事前学習期間同様にirisデータセットです。<br>\n",
    "<br>\n",
    "2値分類としたいため、以下の2つの目的変数のみ利用します。特徴量は4種類全て使います。<br>\n",
    "<br>\n",
    "- virgicolorとvirginica<br>\n",
    "\n",
    "<br>\n",
    "残り2つは特徴量が2つのデータセットを人工的に用意します。以下のコードで説明変数X,目的変数yが作成可能です。「シンプルデータセット1」「シンプルデータセット2」とします。特徴量が2つであるため可視化が容易です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シンプルデータセット1作成コード\n",
    "\n",
    "np.random.seed(seed=0)\n",
    "n_samples = 500\n",
    "f0 = [-1, 2]\n",
    "f1 = [2, -1]\n",
    "cov = [[1.0,0.8], [0.8, 1.0]]\n",
    "f0 = np.random.multivariate_normal(f0, cov, int(n_samples/2))\n",
    "f1 = np.random.multivariate_normal(f1, cov, int(n_samples/2))\n",
    "X1 = np.concatenate((f0, f1))\n",
    "y1 = np.concatenate((np.ones((int(n_samples/2))), np.ones((int(n_samples/2))) *(-1))).astype(np.int)\n",
    "random_index = np.random.permutation(np.arange(n_samples))\n",
    "X1 = X1[random_index]\n",
    "y1 = y1[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シンプルデータセット2作成コード\n",
    "\n",
    "X2 = np.array([[-0.44699 , -2.8073  ],[-1.4621  , -2.4586  ],\n",
    "           [ 0.10645 ,  1.9242  ],[-3.5944  , -4.0112  ],\n",
    "           [-0.9888  ,  4.5718  ],[-3.1625  , -3.9606  ],\n",
    "           [ 0.56421 ,  0.72888 ],[-0.60216 ,  8.4636  ],\n",
    "           [-0.61251 , -0.75345 ],[-0.73535 , -2.2718  ],\n",
    "           [-0.80647 , -2.2135  ],[ 0.86291 ,  2.3946  ],\n",
    "           [-3.1108  ,  0.15394 ],[-2.9362  ,  2.5462  ],\n",
    "           [-0.57242 , -2.9915  ],[ 1.4771  ,  3.4896  ],\n",
    "           [ 0.58619 ,  0.37158 ],[ 0.6017  ,  4.3439  ],\n",
    "           [-2.1086  ,  8.3428  ],[-4.1013  , -4.353   ],\n",
    "           [-1.9948  , -1.3927  ],[ 0.35084 , -0.031994],\n",
    "           [ 0.96765 ,  7.8929  ],[-1.281   , 15.6824  ],\n",
    "           [ 0.96765 , 10.083   ],[ 1.3763  ,  1.3347  ],\n",
    "           [-2.234   , -2.5323  ],[-2.9452  , -1.8219  ],\n",
    "           [ 0.14654 , -0.28733 ],[ 0.5461  ,  5.8245  ],\n",
    "           [-0.65259 ,  9.3444  ],[ 0.59912 ,  5.3524  ],\n",
    "           [ 0.50214 , -0.31818 ],[-3.0603  , -3.6461  ],\n",
    "           [-6.6797  ,  0.67661 ],[-2.353   , -0.72261 ],\n",
    "           [ 1.1319  ,  2.4023  ],[-0.12243 ,  9.0162  ],\n",
    "           [-2.5677  , 13.1779  ],[ 0.057313,  5.4681  ]])\n",
    "y2 = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】 分類問題を解くコードの作成\n",
    "上記3種類の手法で3種類のデータセットを学習・推定するコードを作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各分類器をimport\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "data = pd.DataFrame(data=iris.data, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "target = pd.DataFrame(data=iris.target, columns=['Species'])\n",
    "\n",
    "df = pd.concat([data, target], axis=1)\n",
    "df = df.loc[df['Species'] !=0, ['sepal_length', 'petal_length', 'Species']]\n",
    "\n",
    "X = np.array(df.iloc[:, :-1])\n",
    "y = np.array(df.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "iris data - ROC AUC score: 0.98222\n",
      "samp1 data - ROC AUC score: 1.00000\n",
      "samp2 data - ROC AUC score: 0.73611\n",
      "\n",
      "\n",
      "SVC\n",
      "iris data - ROC AUC score: 1.00000\n",
      "samp1 data - ROC AUC score: 1.00000\n",
      "samp2 data - ROC AUC score: 0.52778\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "iris data - ROC AUC score: 0.83333\n",
      "samp1 data - ROC AUC score: 1.00000\n",
      "samp2 data - ROC AUC score: 0.41667\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg = SGDClassifier(loss='log')\n",
    "svc = SVC(probability=True)\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "database = [[X, y],\n",
    "            [X1, y1],\n",
    "            [X2, y2]]\n",
    "d_labels = ['iris', 'samp1', 'samp2']\n",
    "models = [log_reg, svc, tree]\n",
    "m_labels = ['Logistic Regression', 'SVC', 'Decision Tree']\n",
    "\n",
    "for model, m_label in zip(models, m_labels):\n",
    "    print(m_label)\n",
    "    \n",
    "    for data, d_label in zip(database, d_labels):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = scratch_train_test_split(data[0], data[1], train_size=0.7)\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict_proba(X_test)\n",
    "\n",
    "        print(\"{} data - ROC AUC score: {:.5f}\"\n",
    "              .format(d_label, roc_auc_score(y_test, pred[:,1])))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 回帰問題\n",
    "回帰は1種類をスクラッチします。<br>\n",
    "\n",
    "- 線形回帰<br>\n",
    "\n",
    "<br>\n",
    "線形回帰は勾配降下法を用いて計算するSGDRegressorクラスを利用してください。<br>\n",
    "データセットは事前学習期間同様にHouse Pricesコンペティションのものを使います。<br>\n",
    "<br>\n",
    "train.csvをダウンロードし、目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】 回帰問題を解くコードの作成\n",
    "線形回帰でHouse Pricesデータセットを学習・推定するコードを作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1710</td>\n",
       "      <td>2003</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262</td>\n",
       "      <td>1976</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1786</td>\n",
       "      <td>2001</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1717</td>\n",
       "      <td>1915</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2198</td>\n",
       "      <td>2000</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GrLivArea  YearBuilt  SalePrice\n",
       "0       1710       2003     208500\n",
       "1       1262       1976     181500\n",
       "2       1786       2001     223500\n",
       "3       1717       1915     140000\n",
       "4       2198       2000     250000"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_data = pd.read_csv('/Users/tamiyagt/Documents/machine learning/02_Kaggle/house prices/train.csv')\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "#GrLivArea、YearBuilt、SalePriceを抽出\n",
    "train = home_data.loc[:, ['GrLivArea', 'YearBuilt', 'SalePrice']]\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor MSE：2.53e+09\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "sgdr = SGDRegressor()\n",
    "\n",
    "X = np.array(train.iloc[:, :-1])\n",
    "y = np.array(train.iloc[:, -1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = scratch_train_test_split(X, y, train_size=0.7, stratify=False)\n",
    "\n",
    "# 標準化処理\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "sgdr.fit(X_train_std, y_train)\n",
    "pred = sgdr.predict(X_test_std)\n",
    "\n",
    "sgdr_mse = mean_squared_error(pred, y_test)\n",
    "\n",
    "print(\"SGDRegressor MSE：{:.3g}\".format(sgdr_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
